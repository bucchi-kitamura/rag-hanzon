from llama_index.llms.ollama import Ollama

def main():
    llm = Ollama(
        model="schroneko/gemma-2-2b-jpn-it"
    )

    while True:
        # ãƒ¦ãƒ¼ã‚¶ãƒ¼ã®å…¥åŠ›å¾…ã¡çŠ¶æ…‹ ã“ã“ã§ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã‚’å…¥åŠ›ã™ã‚‹
        user_input = input("ã‚ãªãŸ: ")

        # çµ‚äº†æ¡ä»¶ ãƒ¦ãƒ¼ã‚¶ãƒ¼ãŒç‰¹å®šã®ã‚­ãƒ¼ãƒ¯ãƒ¼ãƒ‰ã‚’å…¥åŠ›ã—ãŸã‚‰å¯¾è©±ã‚’çµ‚äº†ã™ã‚‹
        if user_input.lower() in ["exit", "çµ‚äº†"]:
            print("ãƒãƒ£ãƒƒãƒˆãƒœãƒƒãƒˆã‚’çµ‚äº†ã—ã¾ã™ã€‚")
            break

        # ç©ºã®çŠ¶æ…‹ã§Enterã‚’æŠ¼ã•ã‚ŒãŸå ´åˆã¯ã‚¹ã‚­ãƒƒãƒ—ã™ã‚‹
        if not user_input.strip():
            continue
        response = llm.complete(user_input)
        # å¿œç­”ã®æœ«å°¾ã«ä½™åˆ†ãªç©ºç™½æ–‡å­—ãŒå…¥ã‚‹ã“ã¨ãŒã‚ã‚‹ã®ã§stripã‚’ä½¿ã£ã¦å‰Šé™¤ã™ã‚‹
        print(f"ğŸ¤–: {response.text.strip()}\n")

if __name__ == "__main__":
    main()